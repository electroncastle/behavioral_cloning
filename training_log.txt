____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
convolution2d_1 (Convolution2D)  (None, 64, 64, 16)    448         convolution2d_input_1[0][0]      
____________________________________________________________________________________________________
activation_1 (Activation)        (None, 64, 64, 16)    0           convolution2d_1[0][0]            
____________________________________________________________________________________________________
maxpooling2d_1 (MaxPooling2D)    (None, 32, 32, 16)    0           activation_1[0][0]               
____________________________________________________________________________________________________
convolution2d_2 (Convolution2D)  (None, 32, 32, 16)    2320        maxpooling2d_1[0][0]             
____________________________________________________________________________________________________
activation_2 (Activation)        (None, 32, 32, 16)    0           convolution2d_2[0][0]            
____________________________________________________________________________________________________
maxpooling2d_2 (MaxPooling2D)    (None, 16, 16, 16)    0           activation_2[0][0]               
____________________________________________________________________________________________________
convolution2d_3 (Convolution2D)  (None, 16, 16, 32)    4640        maxpooling2d_2[0][0]             
____________________________________________________________________________________________________
activation_3 (Activation)        (None, 16, 16, 32)    0           convolution2d_3[0][0]            
____________________________________________________________________________________________________
convolution2d_4 (Convolution2D)  (None, 16, 16, 32)    9248        activation_3[0][0]               
____________________________________________________________________________________________________
activation_4 (Activation)        (None, 16, 16, 32)    0           convolution2d_4[0][0]            
____________________________________________________________________________________________________
maxpooling2d_3 (MaxPooling2D)    (None, 8, 8, 32)      0           activation_4[0][0]               
____________________________________________________________________________________________________
dropout_1 (Dropout)              (None, 8, 8, 32)      0           maxpooling2d_3[0][0]             
____________________________________________________________________________________________________
convolution2d_5 (Convolution2D)  (None, 8, 8, 64)      18496       dropout_1[0][0]                  
____________________________________________________________________________________________________
activation_5 (Activation)        (None, 8, 8, 64)      0           convolution2d_5[0][0]            
____________________________________________________________________________________________________
convolution2d_6 (Convolution2D)  (None, 8, 8, 64)      36928       activation_5[0][0]               
____________________________________________________________________________________________________
activation_6 (Activation)        (None, 8, 8, 64)      0           convolution2d_6[0][0]            
____________________________________________________________________________________________________
maxpooling2d_4 (MaxPooling2D)    (None, 4, 4, 64)      0           activation_6[0][0]               
____________________________________________________________________________________________________
dropout_2 (Dropout)              (None, 4, 4, 64)      0           maxpooling2d_4[0][0]             
____________________________________________________________________________________________________
convolution2d_7 (Convolution2D)  (None, 4, 4, 128)     73856       dropout_2[0][0]                  
____________________________________________________________________________________________________
activation_7 (Activation)        (None, 4, 4, 128)     0           convolution2d_7[0][0]            
____________________________________________________________________________________________________
convolution2d_8 (Convolution2D)  (None, 2, 2, 128)     147584      activation_7[0][0]               
____________________________________________________________________________________________________
activation_8 (Activation)        (None, 2, 2, 128)     0           convolution2d_8[0][0]            
____________________________________________________________________________________________________
maxpooling2d_5 (MaxPooling2D)    (None, 1, 1, 128)     0           activation_8[0][0]               
____________________________________________________________________________________________________
dropout_3 (Dropout)              (None, 1, 1, 128)     0           maxpooling2d_5[0][0]             
____________________________________________________________________________________________________
flatten_1 (Flatten)              (None, 128)           0           dropout_3[0][0]                  
____________________________________________________________________________________________________
dense_1 (Dense)                  (None, 128)           16512       flatten_1[0][0]                  
____________________________________________________________________________________________________
dropout_4 (Dropout)              (None, 128)           0           dense_1[0][0]                    
____________________________________________________________________________________________________
dense_2 (Dense)                  (None, 96)            12384       dropout_4[0][0]                  
____________________________________________________________________________________________________
dropout_5 (Dropout)              (None, 96)            0           dense_2[0][0]                    
____________________________________________________________________________________________________
dense_3 (Dense)                  (None, 64)            6208        dropout_5[0][0]                  
____________________________________________________________________________________________________
dense_4 (Dense)                  (None, 10)            650         dense_3[0][0]                    
____________________________________________________________________________________________________
dense_5 (Dense)                  (None, 2)             22          dense_4[0][0]                    
====================================================================================================
Total params: 329,296
Trainable params: 329,296
Non-trainable params: 0


334336/334589 [============================>.] - ETA: 0s - loss: 0.0643Epoch 00000: val_loss improved from inf to 0.04601, saving model to model.00-0.05.h5
334589/334589 [==============================] - 64s - loss: 0.0643 - val_loss: 0.0460
Epoch 2/30
334336/334589 [============================>.] - ETA: 0s - loss: 0.0472Epoch 00001: val_loss improved from 0.04601 to 0.04112, saving model to model.01-0.04.h5
334589/334589 [==============================] - 42s - loss: 0.0472 - val_loss: 0.0411
Epoch 3/30
334336/334589 [============================>.] - ETA: 0s - loss: 0.0439Epoch 00002: val_loss improved from 0.04112 to 0.04002, saving model to model.02-0.04.h5
334589/334589 [==============================] - 44s - loss: 0.0439 - val_loss: 0.0400
Epoch 4/30
334336/334589 [============================>.] - ETA: 0s - loss: 0.0419Epoch 00003: val_loss improved from 0.04002 to 0.03806, saving model to model.03-0.04.h5
334589/334589 [==============================] - 45s - loss: 0.0419 - val_loss: 0.0381
Epoch 5/30
334336/334589 [============================>.] - ETA: 0s - loss: 0.0405Epoch 00004: val_loss improved from 0.03806 to 0.03672, saving model to model.04-0.04.h5
334589/334589 [==============================] - 45s - loss: 0.0405 - val_loss: 0.0367
Epoch 6/30
334336/334589 [============================>.] - ETA: 0s - loss: 0.0394Epoch 00005: val_loss improved from 0.03672 to 0.03548, saving model to model.05-0.04.h5
334589/334589 [==============================] - 43s - loss: 0.0394 - val_loss: 0.0355
Epoch 7/30
334336/334589 [============================>.] - ETA: 0s - loss: 0.0384Epoch 00006: val_loss improved from 0.03548 to 0.03486, saving model to model.06-0.03.h5
334589/334589 [==============================] - 42s - loss: 0.0384 - val_loss: 0.0349
Epoch 8/30
334336/334589 [============================>.] - ETA: 0s - loss: 0.0375Epoch 00007: val_loss improved from 0.03486 to 0.03374, saving model to model.07-0.03.h5
334589/334589 [==============================] - 42s - loss: 0.0375 - val_loss: 0.0337
Epoch 9/30
334336/334589 [============================>.] - ETA: 0s - loss: 0.0367Epoch 00008: val_loss improved from 0.03374 to 0.03258, saving model to model.08-0.03.h5
334589/334589 [==============================] - 43s - loss: 0.0367 - val_loss: 0.0326
Epoch 10/30
334336/334589 [============================>.] - ETA: 0s - loss: 0.0358Epoch 00009: val_loss improved from 0.03258 to 0.03181, saving model to model.09-0.03.h5
334589/334589 [==============================] - 43s - loss: 0.0358 - val_loss: 0.0318
Epoch 11/30
334336/334589 [============================>.] - ETA: 0s - loss: 0.0348Epoch 00010: val_loss improved from 0.03181 to 0.03058, saving model to model.10-0.03.h5
334589/334589 [==============================] - 43s - loss: 0.0348 - val_loss: 0.0306
Epoch 12/30
334336/334589 [============================>.] - ETA: 0s - loss: 0.0337Epoch 00011: val_loss improved from 0.03058 to 0.02901, saving model to model.11-0.03.h5
334589/334589 [==============================] - 42s - loss: 0.0337 - val_loss: 0.0290
Epoch 13/30
334336/334589 [============================>.] - ETA: 0s - loss: 0.0328Epoch 00012: val_loss did not improve
334589/334589 [==============================] - 42s - loss: 0.0328 - val_loss: 0.0292
Epoch 14/30
334336/334589 [============================>.] - ETA: 0s - loss: 0.0320Epoch 00013: val_loss did not improve
334589/334589 [==============================] - 43s - loss: 0.0320 - val_loss: 0.0291
Epoch 15/30
334336/334589 [============================>.] - ETA: 0s - loss: 0.0314Epoch 00014: val_loss improved from 0.02901 to 0.02710, saving model to model.14-0.03.h5
334589/334589 [==============================] - 43s - loss: 0.0314 - val_loss: 0.0271
Epoch 16/30
334336/334589 [============================>.] - ETA: 0s - loss: 0.0309Epoch 00015: val_loss improved from 0.02710 to 0.02682, saving model to model.15-0.03.h5
334589/334589 [==============================] - 42s - loss: 0.0309 - val_loss: 0.0268
Epoch 17/30
334336/334589 [============================>.] - ETA: 0s - loss: 0.0304Epoch 00016: val_loss improved from 0.02682 to 0.02644, saving model to model.16-0.03.h5
334589/334589 [==============================] - 42s - loss: 0.0304 - val_loss: 0.0264
Epoch 18/30
334336/334589 [============================>.] - ETA: 0s - loss: 0.0301Epoch 00017: val_loss did not improve
334589/334589 [==============================] - 43s - loss: 0.0301 - val_loss: 0.0267
Epoch 19/30
334336/334589 [============================>.] - ETA: 0s - loss: 0.0297Epoch 00018: val_loss improved from 0.02644 to 0.02565, saving model to model.18-0.03.h5
334589/334589 [==============================] - 43s - loss: 0.0297 - val_loss: 0.0256
Epoch 20/30
334336/334589 [============================>.] - ETA: 0s - loss: 0.0294Epoch 00019: val_loss improved from 0.02565 to 0.02546, saving model to model.19-0.03.h5
334589/334589 [==============================] - 43s - loss: 0.0294 - val_loss: 0.0255
Epoch 21/30
334336/334589 [============================>.] - ETA: 0s - loss: 0.0291Epoch 00020: val_loss did not improve
334589/334589 [==============================] - 42s - loss: 0.0291 - val_loss: 0.0256
Epoch 22/30
334336/334589 [============================>.] - ETA: 0s - loss: 0.0288Epoch 00021: val_loss improved from 0.02546 to 0.02497, saving model to model.21-0.02.h5
334589/334589 [==============================] - 43s - loss: 0.0288 - val_loss: 0.0250
Epoch 23/30
334336/334589 [============================>.] - ETA: 0s - loss: 0.0286Epoch 00022: val_loss improved from 0.02497 to 0.02471, saving model to model.22-0.02.h5
334589/334589 [==============================] - 43s - loss: 0.0286 - val_loss: 0.0247
Epoch 24/30
334336/334589 [============================>.] - ETA: 0s - loss: 0.0283Epoch 00023: val_loss improved from 0.02471 to 0.02454, saving model to model.23-0.02.h5
334589/334589 [==============================] - 43s - loss: 0.0283 - val_loss: 0.0245
Epoch 25/30
334336/334589 [============================>.] - ETA: 0s - loss: 0.0282Epoch 00024: val_loss improved from 0.02454 to 0.02432, saving model to model.24-0.02.h5
334589/334589 [==============================] - 43s - loss: 0.0282 - val_loss: 0.0243
Epoch 26/30
334336/334589 [============================>.] - ETA: 0s - loss: 0.0279Epoch 00025: val_loss did not improve
334589/334589 [==============================] - 43s - loss: 0.0279 - val_loss: 0.0245
Epoch 27/30
334336/334589 [============================>.] - ETA: 0s - loss: 0.0277Epoch 00026: val_loss improved from 0.02432 to 0.02399, saving model to model.26-0.02.h5
334589/334589 [==============================] - 42s - loss: 0.0277 - val_loss: 0.0240
Epoch 28/30
334336/334589 [============================>.] - ETA: 0s - loss: 0.0276Epoch 00027: val_loss did not improve
334589/334589 [==============================] - 42s - loss: 0.0276 - val_loss: 0.0243
Epoch 29/30
334336/334589 [============================>.] - ETA: 0s - loss: 0.0274Epoch 00028: val_loss improved from 0.02399 to 0.02361, saving model to model.28-0.02.h5
334589/334589 [==============================] - 43s - loss: 0.0274 - val_loss: 0.0236
Epoch 30/30
334336/334589 [============================>.] - ETA: 0s - loss: 0.0272Epoch 00029: val_loss did not improve
334589/334589 [==============================] - 43s - loss: 0.0272 - val_loss: 0.0243
Training time: 22:0 min:sec

